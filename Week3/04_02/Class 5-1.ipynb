{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492 전산학특강<인공지능 산업 및 스마트에너지>\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Recurrent nerural netowrks (RNNs)\n",
    "## Schedule of this week\n",
    "5. **Basic of RNNs** <br>\n",
    "    5-1. Introduction to RNNs <br>\n",
    "    5-2. Word embeddings <br>\n",
    "    5-3. Text classification with an RNN\n",
    "    \n",
    "1. **Appling RNNs to Other Domains** <br>\n",
    "    6-1. Text generation with an RNN <br>\n",
    "    6-2. Time series forecasting <br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Basic of RNNs\n",
    "### 5-1. Introduction to RNNs\n",
    "#### Basic RNN structure \n",
    "\n",
    "**Why the RNNs?** <br>\n",
    "Traditional neural networks such as FFNN, CNN have no **persistence properties** when predicting some results (e.g., as you read this essay, you understand each word based on your understanding of previous words). This is a major shortcoming of the neural networks. \n",
    "\n",
    "<img src=https://miro.medium.com/max/5412/1*NKhwsOYNUT5xU7Pyf6Znhg.png>\n",
    "\n",
    "Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. RNN is a class of artificial neural network where connections between units form a directed graph along a **temporal sequence**. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (**memory**) to process sequences of inputs. This makes them applicable to tasks such as sentence generation, machine translation, image captioning or speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even though RNNs are quite powerful, they suffer from **vanishing gradient problem** which hinders them from using long-term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradient problem\n",
    "\n",
    "<img src=https://miro.medium.com/max/2288/1*lTeIFg5Ecl0hMd3FeNGDYA.png>\n",
    "\n",
    "\n",
    "\n",
    "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. \n",
    "\n",
    "The problem is that in some cases, **the gradient will be vanishingly small**, effectively preventing the weight from changing its value. **In the worst case**, this **may completely stop the neural network from further training**. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range `(0, 1)`, and backpropagation computes gradients by the chain rule. This has the effect of **multiplying $n$ of these small numbers to compute gradients of the \"front\" layers in an $n$-layer network**, meaning that the gradient (error signal) decreases exponentially with $n$ while the front layers train very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exmaple of vanishing gradient**\n",
    "\n",
    "<img src=https://miro.medium.com/max/1550/1*qKjN8eiiW46Xs18JTGLeaA.png>\n",
    "\n",
    "- Sentence 1: That **_cat_**, which already ate ..., **_was_** full. \n",
    "- Sentence 2: That **_cats_**, which already ate ..., **_were_** full. \n",
    "\n",
    "This is one example when language can have **long term dependencies**. like in above example **cat** which came early in the sentence can affect what came later.\n",
    "The basic RNN is not very good in remembering long term dependencies.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1550/1*Kl3zMpEfTe7zbk4XKXiB_Q.png>\n",
    "\n",
    "Basic RNN model has many **local influences** because of Recurrent Neural networks as the earlier information. Meaning the output is mostly affected by the value close to it.\n",
    "Thus, $y^{<3>}$ is mainly influenced by value close to $y^{<3>}$ ($x^{<1>},x^{<>},x^{<3>}$).\n",
    "\n",
    "In the case of $y^{<T_y>}$, this cannot be influenced by the early inputs in the sequences ($x^{<1>},x^{<>},x^{<3>}$). It has hard for the error to back propagate to the beginning of the sequence. This is the weakness of Basic RNN.\n",
    "\n",
    "**How to Solve the Vanishing Gradients:**\n",
    " [LSTM(Long Short Term Memory)](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM) or [GRU(gated recurrent units)](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU) both are a very effective solution for addressing the vanishing gradient problem and will allow our neural network to capture much longer range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long short-term memory (LSTM)\n",
    "\n",
    "<img src=https://www.foundationai.org/wp-content/uploads/2018/08/948d40857c4118bc7aebffda6e5f4e57.png>\n",
    "\n",
    "Long Short Term Memory networks – usually just called \"LSTMs\" – are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior.\n",
    "\n",
    "A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate** and a **forget gate**. The cell is responsible for _\"remembering\" values over arbitrary time intervals_; hence the word \"memory\" in LSTM. Each of the three gates can be thought intuitively as regulators of the flow of values that goes through the connections of the LSTM. There are connections between these gates and the cell.\n",
    "\n",
    "\n",
    "Reading material for more detailed LSTM:\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differnet types of RNNs\n",
    "\n",
    "![Differnt types of RNNs](images/rnn-types.jpg)\n",
    "\n",
    "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state.\n",
    "\n",
    "From left to right:\n",
    "- One to one: Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).\n",
    "- One to many: Sequence output (e.g. image captioning takes an image and outputs a sentence of words).\n",
    "- Many to one: Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment).\n",
    "- Many to many: Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).\n",
    "- Many to many: Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).\n",
    "\n",
    "Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
