{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492 전산학특강<인공지능 산업 및 스마트에너지>\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning with pre-trained CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Several well-known CNN models\n",
    "\n",
    "Constructing and training your own CNN models from scratch can be hard and a long task. A common trick used in deep learning is to use a **pre-trained** model and **fine-tune** it to the specific data it will be used for. \n",
    "\n",
    "To use the pre-trained models for our task, we will first look into several well-known CNN models. Many CNN models have been studied since the 1990s. Especially, since 2010, more advanced models have been developed  through a [ImageNet: Large scale visual recognition challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) in the computer vision fields such as image recognition, object detection, etc.\n",
    "\n",
    "- LeNet \n",
    "- AlexNet\n",
    "- VGG \n",
    "- MobileNet\n",
    "- Inception (GoogLeNet)\n",
    "- ResNet50 \n",
    "- Xception\n",
    "- ... more to come"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet\n",
    "\n",
    "<img src=https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png width=\"700\">\n",
    "\n",
    "- Yann LeCun et al. proposed a neural network architecture for handwritten and machine-printed character recognition in 1990’s.\n",
    "- The first successful applications of CNN.\n",
    "- This model consists of 3 convolution layers, 2 pooling layers and 1 fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet\n",
    "\n",
    "<img src=https://miro.medium.com/max/3072/1*qyc21qM0oxWEuRaj-XJKcw.png width=\"700\">\n",
    "\n",
    "- The first work that popularized convolutional neural networks in computer vision.\n",
    "- This was submitted to the ImageNet ILSVRC challenge in 2012. \n",
    "- This network had a very similar architecture to LeNet, but was deeper, bigger, and featured convolutional layers stacked on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG\n",
    "\n",
    "##### VGG-16\n",
    "<img src=https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg width=\"500\">\n",
    "\n",
    "##### VGG-19\n",
    "<img src=https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg width=\"500\">\n",
    "\n",
    "- The runner-up in ILSVRC 2014 (VGG16)\n",
    "- Its main contribution was in showing that the depth of the network is a critical component for good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception(-v3) (GoogLeNet)\n",
    "\n",
    "<img src=https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png width=\"750\">\n",
    "\n",
    "- The winner in ILSVRC 2014\n",
    "- Its main contribution was the development of an `Inception Module` that dramatically reduced the number of parameters in the network.\n",
    "- There are also several follow-up versions to the GoogLeNet, most recently Inception-v4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet(-50)\n",
    "\n",
    "<img src=https://i.stack.imgur.com/XTo6Q.png width=\"300\">\n",
    "\n",
    "- The winner in ILSVRC 2015\n",
    "- It features special skip connections and a heavy use of batch normalization.\n",
    "- The architecture is also missing fully connected layers at the end of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Tutorial of the pre-trained models using Keras API\n",
    "\n",
    "#### VGG16\n",
    "We can use the pre-trained CNN models mentioned above using the Keras API [tf.keras.applications](https://keras.io/applications/).\n",
    "\n",
    "(More models available in Kareas can be found here: https://github.com/keras-team/keras-applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the pre-trained VGG16 model has not been downloaded, the below cell will download the pre-trained model first, then load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG net using Keras API \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_root = '/gdrive/My Drive'\n",
    "print('In gdrive :', os.listdir(gdrive_root))\n",
    "\n",
    "notebook_dir = os.path.join(gdrive_root, 'Colab Notebooks')\n",
    "print('In Colab Notebooks :', os.listdir(notebook_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTtmYIYGx1zXVT4UyjturF3uL7AWx4FKdfJJAnjCrPKxJnjHSxJ -O '/gdrive/My Drive/strawberry.png'\n",
    "print('In gdrive :', os.listdir(gdrive_root))\n",
    "\n",
    "image_path = os.path.join(gdrive_root, 'strawberry.png')\n",
    "img = Image.open(image_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change image size to (224,224) \n",
    "\n",
    "# conver image to array (224, 224, 3)\n",
    "\n",
    "# change image dim from (224, 224, 3) to (1, 224, 224, 3)\n",
    "\n",
    "# feed the image to vgg model \n",
    "\n",
    "print('Input image shape:', x.shape, end='\\n\\n')\n",
    "\n",
    "# predict using VGG Net (result shape is (1,1000))\n",
    "predictions = \n",
    "\n",
    "print('Top-{} predictions:'.format(len(predictions)))\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print('{}. {}'.format(index + 1, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shonw in the prediction results, the VGG16 model predicted a class of the input as a _'strawberry'_ with highest confidence value (or probability), 0.999. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict again with another image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ww.namu.la/s/a9c9d9c03c04786189170e15a45360e0e310fdf789723fe314ea6910f56d609bbca103e6b735ff8c7dec82fc9db7363b64bb507fca3b2a4fb530e28e771f98834ce60900851ec55503934dd4bdf7d183870d10247f4597014fb8c27082bafa29 -O '/gdrive/My Drive/orange.png'\n",
    "\n",
    "image_path = os.path.join(gdrive_root, 'orange.png')\n",
    "img = Image.open(image_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "x = tf.keras.preprocessing.image.img_to_array(image)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "print('Input image shape:', x.shape, end='\\n\\n')\n",
    "\n",
    "predictions = vgg16(x)\n",
    "predictions = tf.keras.applications.vgg16.decode_predictions(predictions.numpy())[0]\n",
    "print('Top-{} predictions:'.format(len(predictions)))\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print('{}. {}'.format(index + 1, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50\n",
    "\n",
    "Similar to VGG16 model, we will just use the Keras API to load the pre-trained ResNet50.\n",
    "\n",
    "A ResNet is composed by two main blocks: **Identity Block** and the **ConvBlock**.\n",
    "- IdentityBlock is the block that has no convolutional layer at shortcut\n",
    "- ConvBlock is the block that has a convolutional layer at shortcut\n",
    "\n",
    "ResNet50 is so big. Let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = tf.keras.applications.ResNet50(include_top=True, weights='imagenet')\n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "x = tf.keras.preprocessing.image.img_to_array(image)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "print('Input image shape:', x.shape, end='\\n\\n')\n",
    "\n",
    "# # predict using ResNet-50\n",
    "predictions = \n",
    "\n",
    "print('Top-{} predictions:'.format(len(predictions)))\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print('{}. {}'.format(index + 1, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - please cite this paper if you use the VGG models in your work.\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - please cite this paper if you use the ResNet model in your work.\n",
    "- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567) - please cite this paper if you use the Inception v3 model in your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
