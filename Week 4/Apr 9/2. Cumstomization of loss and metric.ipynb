{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492 전산학특강<인공지능 산업 및 스마트에너지>\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2. Cumstomization of loss and metric\n",
    "To train a model with `fit`, you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n",
    "You pass these to the model as arguments to the compile() method:\n",
    "```python\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "```\n",
    "If your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate to contribution of each output to the total loss of the model. \n",
    "\n",
    "Note that in many cases, the loss and metrics are specified via string identifiers, as a shortcut:\n",
    "```python\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "```\n",
    "\n",
    "For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many built-in optimizers, losses, and metrics are available\n",
    "In general, you won't have to create from scratch your own losses, metrics, or optimizers, because what you need is likely already part of the Keras API:\n",
    "\n",
    "- `Optimizers`: - `SGD()` (with or without momentum) - `RMSprop()` - `Adam()` - etc.\n",
    "- `Losses`: - `MeanSquaredError()` - `KLDivergence()` - `CosineSimilarity()` - etc.\n",
    "- `Metrics`: - `AUC()` - `Precision()` - `Recall()` - etc.\n",
    "\n",
    "\n",
    "#### Custom losses\n",
    "There are two ways to provide **custom losses with Keras**. The first example creates a function that accepts inputs `y_true` and `y_pred`. The following example shows a loss function that computes the average distance between the real data and the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a toy dataset for the sake of this example\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom loss function\n",
    "def basic_loss_function(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(y_true - y_pred)\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "# use the custom loss funciton we defined\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=basic_loss_function,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can subclass the `tf.keras.losses.Loss` class** and implement the following two methods:\n",
    "- `__init__(self)`: Accept parameters to pass during the call of your loss function\n",
    "- `call(self, y_true, y_pred)`: Use the targets (`y_true`) and the model predictions (`y_pred`) to compute the model's loss\n",
    "\n",
    "The following example shows how to implement a `WeightedCrossEntropy` loss function that calculates a BinaryCrossEntropy loss, where the loss of a certain class or the whole function can be modified by a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossEntropy(keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      pos_weight: Scalar to affect the positive labels of the loss function.\n",
    "      weight: Scalar to affect the entirety of the loss function.\n",
    "      from_logits: Whether to compute loss form logits or the probability.\n",
    "      reduction: Type of tf.keras.losses.Reduction to apply to loss.\n",
    "      name: Name of the loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight, weight, from_logits=False,\n",
    "                 reduction=keras.losses.Reduction.AUTO,\n",
    "                 name='weighted_binary_crossentropy'):\n",
    "        super(WeightedBinaryCrossEntropy, self).__init__(reduction=reduction,\n",
    "                                                         name=name)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if not self.from_logits:\n",
    "            # Manually calculate the weighted cross entropy.\n",
    "            # Formula is qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
    "            # where z are labels, x is logits, and q is the weight.\n",
    "            # Since the values passed are from sigmoid (assuming in this case)\n",
    "            # sigmoid(x) will be replaced by y_pred\n",
    "\n",
    "            # qz * -log(sigmoid(x)) 1e-6 is added as an epsilon to stop passing a zero into the log\n",
    "            x_1 = y_true * self.pos_weight * -tf.math.log(y_pred + 1e-6)\n",
    "\n",
    "            # (1 - z) * -log(1 - sigmoid(x)). Epsilon is added to prevent passing a zero into the log\n",
    "            x_2 = (1 - y_true) * -tf.math.log(1 - y_pred + 1e-6)\n",
    "\n",
    "            return tf.add(x_1, x_2) * self.weight \n",
    "\n",
    "        # Use built in function\n",
    "        return tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.pos_weight) * self.weight\n",
    "\n",
    "\n",
    "# use the loss function you defined\n",
    "model.compile(\n",
    "\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom metrics\n",
    "If you need a metric that isn't part of the API, **you can easily create custom metrics by subclassing the Metric class**. You will need to implement 4 methods:\n",
    "- `__init__(self)`: state variables for your metric.\n",
    "- `update_state(self, y_true, y_pred, sample_weight=None)`: the targets `y_true` and the model predictions `y_pred` to update the state variables.\n",
    "- `result(self)`: the state variables to compute the final results.\n",
    "- `reset_states(self)`: reinitializes the state of the metric.\n",
    "\n",
    "State update and results computation are kept separate (in `update_state()` and `result()`, respectively) because in some cases, results computation might be very expensive, and would only be done periodically.\n",
    "\n",
    "Here's a simple example showing how to implement a `CatgoricalTruePositives` metric, that counts how many samples where correctly classified as belonging to a given class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatgoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name='categorical_true_positives', **kwargs):\n",
    "        super(CatgoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, 'int32') == tf.cast(y_pred, 'int32')\n",
    "        values = tf.cast(values, 'float32')\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, 'float32')\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.)\n",
    "\n",
    "\n",
    "        \n",
    "model.compile(\n",
    "\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the loss and metric we defined to multi-input, multi-output models\n",
    "Consider the following model, which has an image input of shape `(32, 32, 3)` (that's `(height, width, channels)`) and a timeseries input of shape `(None, 10)` (that's `(timesteps, features)`). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape `(1,)`) and a probability distribution over five classes (of shape `(5,)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_input =\n",
    "timeseries_input = \n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = \n",
    "class_output = \n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this model, so you can clearly see what we're doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At compilation time, **we can specify different losses to different ouptuts**, by passing the loss functions as a list:\n",
    "\n",
    "``` python\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])\n",
    "```\n",
    "\n",
    "If we only passed a single loss function to the model, the same loss function would be applied to every output, which is not appropriate here.\n",
    "\n",
    "Since we gave names to our output layers, we coud also specify per-output losses and metrics via a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the **use of explicit names and dicts** if you have more than 2 outputs.\n",
    "\n",
    "It's possible to give **different weights to different output-specific losses** (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), using the loss_weight argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy Numpy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "\"\"\"\n",
    "# Alernatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the `Dataset` use case: similarly as what we did for Numpy arrays, the `Dataset` should return a **tuple of dicts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'img_input': img_data, 'ts_input': ts_data},\n",
    "     {'score_output': score_targets, 'class_output': class_targets}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-3. Several options for optimizer and training\n",
    "#### Using sample weighting and class weighting\n",
    "Besides input data and target data, it is possible to pass **sample weights** or **class weights** to a model when using fit:\n",
    "\n",
    "- When training from Numpy data: via the `sample_weight` and `class_weight` arguments.\n",
    "- When training from Datasets: by having the Dataset return a tuple `(input_batch, target_batch, sample_weight_batch)`.\n",
    "\n",
    "A **_\"sample weights\"_** array is an array of numbers that specify **how much weight each sample in a batch should have in computing the total loss.** **It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes)**. When the **weights used are ones and zeros**, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss).\n",
    "\n",
    "A **_\"class weights\"_** dict is a more specific instance of the same concept: it maps class indices to the sample weight that should be used for samples belonging to this class. For instance, if class \"0\" is twice less represented than class \"1\" in your data, you could use class_weight={0: 1., 1: 0.5}.\n",
    "\n",
    "Here's a Numpy example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                # Set weight \"2\" for class \"5\",\n",
    "                # making this class 2x more important\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "print('Fit with class weight')\n",
    "model.fit(\n",
    "    \n",
    ")\n",
    "\n",
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "print('\\nFit with sample weight')\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a matching `Dataset` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using callbacks \n",
    "\n",
    "Callbacks in Keras are objects that are called at different point during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.) and which can be used to implement behaviors such as:\n",
    "\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded\n",
    "- Etc.\n",
    "\n",
    "**Many built-in callbacks are available:**\n",
    "- `ModelCheckpoint`: Periodically save the model.\n",
    "- `EarlyStopping`: Stop training when training is no longer improving the validation metrics.\n",
    "- `TensorBoard`: periodically write model logs that can be visualized in TensorBoard (more details in the section \"Visualization\").\n",
    "- `CSVLogger`: streams loss and metrics data to a CSV file.\n",
    "- etc.\n",
    "\n",
    "Callbacks can be passed as a list to your call to `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "model.fit(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.\n",
    "\n",
    "The easiest way to achieve this is with the ModelCheckpoint callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    \n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing your own callback** <br>\n",
    "You can create a custom callback by extending the base class `keras.callbacks.Callback`. A callback has access to its associated model through the class property `self.model`.\n",
    "\n",
    "Here's a simple example saving a list of per-batch loss values during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You call also write your own callback for saving and restoring models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using learning rate schedules\n",
    "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. **This is generally known as \"_learning rate decay_\".**\n",
    "\n",
    "The learning decay schedule could be **static** (fixed in advance, as a function of the current epoch or the current batch index), or **dynamic** (responding to the current behavior of the model, in particular the validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passing a schedule to an optimizer** <br>\n",
    "You can easily use a _static learning rate decay_ schedule by passing a schedule object as the learning_rate argument in your optimizer:\n",
    "[`tf.keras.optimizers.schedules.ExponentialDecay`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay): A LearningRateSchedule that uses an exponential decay schedule.\n",
    "- `initial_learning_rate`: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.\n",
    "- `decay_steps`: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.\n",
    "- `decay_rate`: A scalar float32 or float64 Tensor or a Python number. The decay rate.\n",
    "- `staircase`: Boolean. If True decay the learning rate at discrete intervals\n",
    "- `name`: String. Optional name of the operation. Defaults to 'ExponentialDecay'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "\n",
    ")\n",
    "\n",
    "# add the learning_rate we defeind to learning_rate argument in optimizer \n",
    "optimizer = keras.optimizers.RMSprop(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several built-in schedules are available: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay`, and `InverseTimeDecay`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using callbacks to implement a dynamic learning rate schedule**\n",
    "A _dynamic learning rate schedule_ (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects since the optimizer does not have access to validation metrics.\n",
    "\n",
    "However, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern **by using a callback that modifies the current learning rate on the optimizer.** In fact, this is even built-in as the `ReduceLROnPlateau` callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing loss and metrics during training\n",
    "The best way to keep an eye on your model during training is to use [`TensorBoard`](https://www.tensorflow.org/tensorboard), a browser-based application that you can run locally that provides you with:\n",
    "\n",
    "- Live plots of the loss and metrics for training and evaluation\n",
    "- (optionally) Visualizations of the histograms of your layer activations\n",
    "- (optionally) 3D visualizations of the embedding spaces learned by your Embedding layers\n",
    "\n",
    "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
    "\n",
    "`tensorboard --logdir=/full_path_to_your_logs`\n",
    "\n",
    "**Using the TensorBoard callback**\n",
    "The easiest way to use TensorBoard with a Keras model and the fit method is the `TensorBoard` callback.\n",
    "\n",
    "In the simplest case, just specify where you want the callback to write logs, and you're good to go:\n",
    "```python\n",
    "tensorboard_cbk = tf.keras.callbacks.TensorBoard(log_dir='/full_path_to_your_logs')\n",
    "model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk])\n",
    "```\n",
    "\n",
    "The `TensorBoard` callback has many useful options, including whether to log embeddings, histograms, and how often to write logs:\n",
    "```python\n",
    "keras.callbacks.TensorBoard(\n",
    "  log_dir='/full_path_to_your_logs',\n",
    "  histogram_freq=0,  # How often to log histogram visualizations\n",
    "  embeddings_freq=0,  # How often to log embedding visualizations\n",
    "  update_freq='epoch')  # How often to write logs (default: once per epoch)\n",
    "```\n",
    "\n",
    "\n",
    "**For more detailed usage about TensorBoard in your notebook, refer to this site:** https://www.tensorflow.org/tensorboard/r2/tensorboard_in_notebooks \n",
    "\n",
    "**If you use Google Colab, use the \"TensorboardColab\" package\"** <br>\n",
    "Installation: !pip install tensorboardcolab in your cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
